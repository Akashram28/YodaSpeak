{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using seq2seq model to reconstruct english sentences into Yodish sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_File_Path = \"./Training_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "### Collecting Tokenized Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['59 19 2359 27\\n',\n",
       " '18506 882 882 2558 59 19 12022 27\\n',\n",
       " '65 59177 428 14 2269 10 96825 72439 550 7\\n',\n",
       " '422 23 161 27\\n',\n",
       " '843 593 161 1256 39 27\\n']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(Training_File_Path,'Training_from_clean_en.txt')) as f:\n",
    "    eng_sents = f.readlines()\n",
    "\n",
    "eng_sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[59, 19, 2359, 27],\n",
       " [18506, 882, 882, 2558, 59, 19, 12022, 27],\n",
       " [65, 59177, 428, 14, 2269, 10, 96825, 72439, 550, 7],\n",
       " [422, 23, 161, 27],\n",
       " [843, 593, 161, 1256, 39, 27]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(eng_sents)):\n",
    "    eng_sents[i] = eng_sents[i][:-1]\n",
    "\n",
    "for i in range(len(eng_sents)):\n",
    "    eng_sents[i] = list(map(int,eng_sents[i].split()))\n",
    "eng_sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akash\\AppData\\Local\\Temp\\ipykernel_13008\\2304852453.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  eng_sents = np.array(eng_sents)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(224028,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_sents = np.array(eng_sents)\n",
    "eng_sents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7690 5 68 22 5 36 34\\n',\n",
       " '17935 5 13647 930 930 987 68 22 5 36 34 12 5 14 4\\n',\n",
       " '25 1729 7 89627 66587 362 55 23813 204 9\\n',\n",
       " '622 5 273 27 5 36 34 13 4\\n',\n",
       " '1326 5 491 492 92 1205 5 36 34\\n']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(Training_File_Path,'Training_to_clean_yoda.txt')) as f:\n",
    "    yodish_sents = f.readlines()\n",
    "\n",
    "yodish_sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7690, 5, 68, 22, 5, 36, 34],\n",
       " [17935, 5, 13647, 930, 930, 987, 68, 22, 5, 36, 34, 12, 5, 14, 4],\n",
       " [25, 1729, 7, 89627, 66587, 362, 55, 23813, 204, 9],\n",
       " [622, 5, 273, 27, 5, 36, 34, 13, 4],\n",
       " [1326, 5, 491, 492, 92, 1205, 5, 36, 34]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(eng_sents)):\n",
    "    yodish_sents[i] = yodish_sents[i][:-1]\n",
    "\n",
    "for i in range(len(yodish_sents)):\n",
    "    yodish_sents[i] = list(map(int,yodish_sents[i].split()))\n",
    "yodish_sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akash\\AppData\\Local\\Temp\\ipykernel_13008\\1551903220.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  yodish_sents = np.array(yodish_sents)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(224028,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yodish_sents = np.array(yodish_sents)\n",
    "yodish_sents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Word-Index tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the\\n',\n",
       " '0\\n',\n",
       " ')\\n',\n",
       " '(\\n',\n",
       " 'to\\n',\n",
       " 'in\\n',\n",
       " '00\\n',\n",
       " 'The\\n',\n",
       " '0000\\n',\n",
       " 'is\\n']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(Training_File_Path,\"vocab250000_from.txt\"), encoding=\"utf-8\") as f:\n",
    "    eng_tokens = f.readlines()\n",
    "eng_tokens[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(eng_tokens)):\n",
    "    eng_tokens[i] = eng_tokens[i][:-1]\n",
    "\n",
    "eng_word_index = pd.DataFrame({\"token\" : eng_tokens})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75046</th>\n",
       "      <td>LAVALIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62753</th>\n",
       "      <td>R-Squared</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10934</th>\n",
       "      <td>witness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32251</th>\n",
       "      <td>Paton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87879</th>\n",
       "      <td>printfair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           token\n",
       "75046    LAVALIN\n",
       "62753  R-Squared\n",
       "10934    witness\n",
       "32251      Paton\n",
       "87879  printfair"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_word_index.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "token    What\n",
       "Name: 59, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_word_index.iloc[59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and\\n',\n",
       " 'Hmmmmmm\\n',\n",
       " 'Yes\\n',\n",
       " 'Yeesssssss\\n',\n",
       " 'hmmm\\n',\n",
       " 'Herh\\n',\n",
       " '0\\n',\n",
       " 'Of\\n',\n",
       " ')\\n',\n",
       " '(\\n']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(Training_File_Path,\"vocab250000_to.txt\"), encoding=\"utf-8\") as f:\n",
    "    yodish_tokens = f.readlines()\n",
    "yodish_tokens[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(yodish_tokens)):\n",
    "    yodish_tokens[i] = yodish_tokens[i][:-1]\n",
    "\n",
    "yodish_word_index = pd.DataFrame({\"token\" : yodish_tokens})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45543</th>\n",
       "      <td>caso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47247</th>\n",
       "      <td>Postulated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5705</th>\n",
       "      <td>Cover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61941</th>\n",
       "      <td>expressway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51937</th>\n",
       "      <td>fiab</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            token\n",
       "45543        caso\n",
       "47247  Postulated\n",
       "5705        Cover\n",
       "61941  expressway\n",
       "51937        fiab"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yodish_word_index.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "token    Light\n",
       "Name: 7690, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yodish_word_index.iloc[7690]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
    "from keras import optimizers\n",
    "\n",
    "def define_model(in_vocab,out_vocab,in_timesteps,out_timesteps,units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(in_vocab,units,input_length=in_timesteps,mask_zero=True))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(RepeatVector(out_timesteps))\n",
    "    model.add(LSTM(units,return_sequences=True))\n",
    "    model.add(Dense(out_vocab,activation=\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204171\n"
     ]
    }
   ],
   "source": [
    "eng_timesteps_length = 15\n",
    "yodish_timesteps_length = 15\n",
    "reduced_eng_sents = []\n",
    "reduced_yodish_sents = []\n",
    "for i in range(len(eng_sents)):\n",
    "    if len(eng_sents[i]) <= eng_timesteps_length and len(yodish_sents[i]) <= yodish_timesteps_length :\n",
    "        reduced_eng_sents.append(eng_sents[i])\n",
    "        reduced_yodish_sents.append(yodish_sents[i])\n",
    "yodish_timesteps_length = max([len(i) for i in reduced_yodish_sents])\n",
    "print(len(reduced_eng_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab_size = eng_word_index.shape[0]\n",
    "yodish_vocab_size = yodish_word_index.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model(eng_vocab_size,yodish_vocab_size,eng_timesteps_length,yodish_timesteps_length,512)\n",
    "rms = optimizers.RMSprop(learning_rate = 0.01)\n",
    "model.compile(optimizer=rms,loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "def padder(sent,length):\n",
    "    sent = pad_sequences(sent,maxlen=length,padding=\"post\")\n",
    "    return sent\n",
    "reduced_eng_sents = padder(reduced_eng_sents, eng_timesteps_length)\n",
    "reduced_yodish_sents = padder(reduced_yodish_sents, yodish_timesteps_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204171, 15)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reduced_yodish_sents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(np.array(reduced_eng_sents[:20000]),np.array(reduced_yodish_sents[:20000]),test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 15)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.npy', 'wb') as f:\n",
    "    np.save(f, x_train)\n",
    "    np.save(f, x_test)\n",
    "    np.save(f, y_train)\n",
    "    np.save(f, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "32/32 [==============================] - 785s 24s/step - loss: 6.5156\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 780s 24s/step - loss: 4.9687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2122b676800>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,batch_size=512,epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(n):\n",
    "  try:\n",
    "    return yodish_word_index.iloc[n].token\n",
    "  except:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 267ms/step\n",
      "[[[7.3029059e-03 4.4489057e-06 4.4394033e-06 ... 4.5469092e-06\n",
      "   4.3572468e-06 4.6465307e-06]\n",
      "  [2.8178504e-01 1.9009400e-06 1.9176150e-06 ... 2.0117582e-06\n",
      "   2.0160912e-06 1.9902195e-06]\n",
      "  [4.5211458e-01 9.1844623e-07 9.2665834e-07 ... 9.7055283e-07\n",
      "   9.8806834e-07 9.6528288e-07]\n",
      "  ...\n",
      "  [6.2330139e-01 3.6240289e-07 3.7096376e-07 ... 3.8983330e-07\n",
      "   3.9673014e-07 3.8405787e-07]\n",
      "  [6.3015640e-01 3.4650549e-07 3.5495307e-07 ... 3.7322675e-07\n",
      "   3.7955783e-07 3.6735523e-07]\n",
      "  [6.3660115e-01 3.3188147e-07 3.4021448e-07 ... 3.5792078e-07\n",
      "   3.6375178e-07 3.5198150e-07]]\n",
      "\n",
      " [[3.9951112e-03 6.3916136e-06 6.4060150e-06 ... 6.5089371e-06\n",
      "   6.2646300e-06 6.6832818e-06]\n",
      "  [2.8626901e-01 1.8916631e-06 1.9092161e-06 ... 1.9994584e-06\n",
      "   2.0047892e-06 1.9802671e-06]\n",
      "  [4.9374646e-01 7.6256202e-07 7.7061480e-07 ... 8.0855057e-07\n",
      "   8.2189217e-07 8.0358768e-07]\n",
      "  ...\n",
      "  [7.3329031e-01 1.5175885e-07 1.5734770e-07 ... 1.6551564e-07\n",
      "   1.6733210e-07 1.6189594e-07]\n",
      "  [7.4118721e-01 1.4027395e-07 1.4557877e-07 ... 1.5301899e-07\n",
      "   1.5475879e-07 1.4966743e-07]\n",
      "  [7.4824250e-01 1.3039235e-07 1.3544945e-07 ... 1.4224199e-07\n",
      "   1.4392060e-07 1.3913179e-07]]\n",
      "\n",
      " [[1.0597251e-03 5.2295854e-06 5.5473142e-06 ... 4.9019441e-06\n",
      "   5.2505238e-06 5.6681301e-06]\n",
      "  [3.2449715e-02 5.2288560e-06 5.2521705e-06 ... 4.8163724e-06\n",
      "   4.9479427e-06 5.2370806e-06]\n",
      "  [1.6069655e-01 3.4219881e-06 3.3764004e-06 ... 3.2385510e-06\n",
      "   3.2194800e-06 3.3601239e-06]\n",
      "  ...\n",
      "  [6.9109476e-01 4.4797838e-07 4.5122118e-07 ... 4.3337189e-07\n",
      "   4.3854433e-07 4.2991405e-07]\n",
      "  [6.9466221e-01 4.3665503e-07 4.3953995e-07 ... 4.2162043e-07\n",
      "   4.2708902e-07 4.1803028e-07]\n",
      "  [6.9710755e-01 4.2818968e-07 4.3097137e-07 ... 4.1304176e-07\n",
      "   4.1861583e-07 4.0936027e-07]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[4.3590716e-03 4.0798323e-06 4.0703108e-06 ... 4.1654766e-06\n",
      "   3.9232032e-06 4.2029537e-06]\n",
      "  [7.1093366e-02 2.9671555e-06 2.9158241e-06 ... 3.1715842e-06\n",
      "   3.0095387e-06 2.9975026e-06]\n",
      "  [1.0627432e-01 2.6712403e-06 2.7428248e-06 ... 2.9739847e-06\n",
      "   2.8328318e-06 2.6546745e-06]\n",
      "  ...\n",
      "  [7.3379695e-01 2.2735087e-07 2.4054916e-07 ... 2.5293957e-07\n",
      "   2.4600266e-07 2.2979971e-07]\n",
      "  [7.3639834e-01 2.2326454e-07 2.3609202e-07 ... 2.4836106e-07\n",
      "   2.4166934e-07 2.2555336e-07]\n",
      "  [7.3845589e-01 2.1988484e-07 2.3242815e-07 ... 2.4461690e-07\n",
      "   2.3806145e-07 2.2202424e-07]]\n",
      "\n",
      " [[7.1291495e-03 4.0128757e-06 3.9898200e-06 ... 4.1015560e-06\n",
      "   3.9176716e-06 4.1847461e-06]\n",
      "  [2.6320758e-01 2.0182219e-06 2.0339016e-06 ... 2.1333549e-06\n",
      "   2.1306203e-06 2.1105245e-06]\n",
      "  [4.2641971e-01 1.0287092e-06 1.0391958e-06 ... 1.0859326e-06\n",
      "   1.1044841e-06 1.0796501e-06]\n",
      "  ...\n",
      "  [5.6545258e-01 5.1208025e-07 5.2035222e-07 ... 5.4453864e-07\n",
      "   5.5693926e-07 5.3957598e-07]\n",
      "  [5.6993520e-01 4.9953468e-07 5.0795558e-07 ... 5.3160943e-07\n",
      "   5.4366114e-07 5.2656770e-07]\n",
      "  [5.7414407e-01 4.8793135e-07 4.9644916e-07 ... 5.1964201e-07\n",
      "   5.3134232e-07 5.1452139e-07]]\n",
      "\n",
      " [[9.5095398e-04 5.9445138e-06 6.2919949e-06 ... 5.5897349e-06\n",
      "   5.9683657e-06 6.4190490e-06]\n",
      "  [2.7175149e-02 5.6168178e-06 5.6375588e-06 ... 5.1894285e-06\n",
      "   5.3278100e-06 5.6226700e-06]\n",
      "  [1.5071325e-01 3.6292977e-06 3.5800467e-06 ... 3.4389461e-06\n",
      "   3.4207631e-06 3.5625644e-06]\n",
      "  ...\n",
      "  [6.9751263e-01 4.3120369e-07 4.3370173e-07 ... 4.1559633e-07\n",
      "   4.2134101e-07 4.1183901e-07]\n",
      "  [6.9934136e-01 4.2358093e-07 4.2621804e-07 ... 4.0818634e-07\n",
      "   4.1383691e-07 4.0438113e-07]\n",
      "  [7.0096415e-01 4.1675600e-07 4.1954345e-07 ... 4.0157772e-07\n",
      "   4.0714031e-07 3.9773849e-07]]] (20, 15, 90941)\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x_test[:20])\n",
    "print(preds,preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 15)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.argmax(preds,axis=-1)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_text = []\n",
    "for i in preds:\n",
    "  temp = []\n",
    "  for j in range(len(i)):\n",
    "    t = get_word(i[j])\n",
    "    # print(t)\n",
    "    if(t == None):\n",
    "      temp.append('')\n",
    "    else:\n",
    "      temp.append(t)\n",
    "  preds_text.append(' '.join(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = []\n",
    "for i in y_test[:20]:\n",
    "  temp = []\n",
    "  for j in range(len(i)):\n",
    "    t = get_word(i[j])\n",
    "    # print(t)\n",
    "    if(t == None):\n",
    "      temp.append('')\n",
    "    else:\n",
    "      temp.append(t)\n",
    "  y_actual.append(' '.join(temp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\"actual\" : y_actual , \"predicted\" : preds_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Created to promote section 00 , several tools ...</td>\n",
       "      <td>• the _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fruit . Hmmmmmm . _PAD _PAD _PAD _PAD _PAD _PA...</td>\n",
       "      <td>• _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Of the sv a model . _PAD _PAD _PAD _PAD _PAD _...</td>\n",
       "      <td>• the _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>More information . . . Www . canfax . ca . _PA...</td>\n",
       "      <td>• _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>• export requirements - netherlands usda . Hmm...</td>\n",
       "      <td>• the _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>E-mail fraud alert  [ 0000-00-00 ] health cana...</td>\n",
       "      <td>The _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mexico export preparedness guide : _PAD _PAD _...</td>\n",
       "      <td>• _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Of agriculture and land reclamation in egypt •...</td>\n",
       "      <td>• the _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Remove the marinated bison from the marinade a...</td>\n",
       "      <td>• the _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>For biotechnology from the us government • con...</td>\n",
       "      <td>• _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               actual  \\\n",
       "9   Created to promote section 00 , several tools ...   \n",
       "6   Fruit . Hmmmmmm . _PAD _PAD _PAD _PAD _PAD _PA...   \n",
       "2   Of the sv a model . _PAD _PAD _PAD _PAD _PAD _...   \n",
       "7   More information . . . Www . canfax . ca . _PA...   \n",
       "13  • export requirements - netherlands usda . Hmm...   \n",
       "17  E-mail fraud alert\n",
       " [ 0000-00-00 ] health cana...   \n",
       "4   Mexico export preparedness guide : _PAD _PAD _...   \n",
       "5   Of agriculture and land reclamation in egypt •...   \n",
       "15  Remove the marinated bison from the marinade a...   \n",
       "18  For biotechnology from the us government • con...   \n",
       "\n",
       "                                            predicted  \n",
       "9   • the _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD ...  \n",
       "6   • _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD...  \n",
       "2   • the _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD ...  \n",
       "7   • _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD...  \n",
       "13  • the _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD ...  \n",
       "17  The _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _P...  \n",
       "4   • _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD...  \n",
       "5   • the _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD ...  \n",
       "15  • the _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD ...  \n",
       "18  • _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
